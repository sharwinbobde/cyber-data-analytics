{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task - Non-Sequential Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from preprocessing import *\n",
    "\n",
    "import glob\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = []\n",
    "for i in range(9, 13):\n",
    "    csvs.extend(glob.glob(f\"data/CTU-13-Dataset/{i}/*.binetflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/CTU-13-Dataset/9/capture20110817.binetflow',\n",
       " 'data/CTU-13-Dataset/10/capture20110818.binetflow',\n",
       " 'data/CTU-13-Dataset/11/capture20110818-2.binetflow',\n",
       " 'data/CTU-13-Dataset/12/capture20110819.binetflow']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for csv in csvs:\n",
    "    dfs.append(pd.read_csv(csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dfs):\n",
    "    # encode labels: botnet=1, background=2, normal=0\n",
    "    dfs[i] = encode_labels(df)\n",
    "    \n",
    "    # combine background and normal flow labels\n",
    "    dfs[i].loc[dfs[i][\"Label\"] == 2, \"Label\"] = 0\n",
    "    \n",
    "    # drop StartTime\n",
    "    dfs[i].drop(columns=[\"StartTime\"], inplace=True)\n",
    "    \n",
    "    # numerically encode features\n",
    "    dfs[i] = encode_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.911384\n",
      "1    0.088616\n",
      "Name: Label, dtype: float64\n",
      "0    0.918802\n",
      "1    0.081198\n",
      "Name: Label, dtype: float64\n",
      "0    0.923879\n",
      "1    0.076121\n",
      "Name: Label, dtype: float64\n",
      "0    0.993339\n",
      "1    0.006661\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    print(df[\"Label\"].value_counts()/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.087508e+06\n",
       "mean     2.945965e+02\n",
       "std      8.375559e+02\n",
       "min      0.000000e+00\n",
       "25%      3.200000e-04\n",
       "50%      9.890000e-04\n",
       "75%      5.064933e+00\n",
       "max      3.600080e+03\n",
       "Name: Dur, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0][\"Dur\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection using Isolation Forests\n",
    "\n",
    "We would like to model the problem of detecting botnets as a anomaly detection task. We believe that this is a sound way to proceed due to the fact that the class imbalance is quite stark as we have seen above. Also, in a realistic scenario, we will not have labelled bot traffic on which to train our models, but what we can do is capture non-malicious traffic in a controlled setting where all nodes are known. This allows us to model non-malicious traffic and then isolate traffic that falls outside this distribution. This is the kind of setup that anomaly detection methods like Isolation Forests have.\n",
    "\n",
    "We want to model the data in two ways:\n",
    "1. Model non-malicious netflows:\n",
    "   - **Train Set:** A random selection of normal and background netflows from the scenarios 9, 10, 11\n",
    "   - **Test Set:** A random selection of netflows from scenarios 9, 10, 11, 12 (including normal, background and botnet)\n",
    "   - We train the model to fit the distribution of non-malicious netflows\n",
    "   - We see the performance of this model on a dataset containing both non-malicious and malicious netflows.\n",
    "   - The hope is that the model can isolate those flows that do not correspond to the non-malicious distribution\n",
    "   - The test set also contains scenario 12 which is not seen during training. This will test the ability of a model to generalize to new scenarios\n",
    "2. Model non-malicious hosts:\n",
    "\n",
    "   - Did not have the time to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dfs = []\n",
    "bot_dfs = []\n",
    "for df in dfs[:-1]:\n",
    "    normal_dfs.append(df.loc[df[\"Label\"] == 0])\n",
    "    bot_dfs.append(df.loc[df[\"Label\"] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_test_df = dfs[-1].loc[dfs[-1][\"Label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_test_flows = bot_test_df.sample(frac=1).reset_index(drop=True).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_flows = pd.concat(normal_dfs).sample(frac=1).reset_index(drop=True).to_numpy()\n",
    "bot_flows = pd.concat(bot_dfs).sample(frac=1).reset_index(drop=True).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(normal_flows)\n",
    "normal_flows = scaler.transform(normal_flows)\n",
    "bot_flows = scaler.transform(bot_flows)\n",
    "bot_test_flows = scaler.transform(bot_test_flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_folds = kfold.split(normal_flows)\n",
    "bot_folds = kfold.split(bot_flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below cell may take a while (2-3 minutes per fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision and Recall for scenario 9, 10, 11\n",
      "0.20280184897045964\n",
      "0.5962003973222484\n",
      "Recall for unseen botnet flows in scenario 12\n",
      "0.5962003973222484\n",
      "Precision and Recall for scenario 9, 10, 11\n",
      "0.20314212006145105\n",
      "0.5849818867798534\n",
      "Recall for unseen botnet flows in scenario 12\n",
      "0.5849818867798534\n",
      "Precision and Recall for scenario 9, 10, 11\n",
      "0.20093424000804608\n",
      "0.6003505843071786\n",
      "Recall for unseen botnet flows in scenario 12\n",
      "0.6003505843071786\n",
      "Precision and Recall for scenario 9, 10, 11\n",
      "0.21069923534908871\n",
      "0.5984808013355593\n",
      "Recall for unseen botnet flows in scenario 12\n",
      "0.5984808013355593\n"
     ]
    }
   ],
   "source": [
    "# y_preds = []\n",
    "# y_valids = []\n",
    "for ((train, norm_valid,), (_, bot_valid)) in zip(normal_folds, bot_folds):\n",
    "    X_train = normal_flows[train]\n",
    "    X_valid = np.vstack((normal_flows[norm_valid], bot_flows[bot_valid]))\n",
    "    y_valid = np.concatenate((np.zeros((normal_flows[norm_valid].shape[0],)), np.ones((bot_flows[bot_valid].shape[0],))))\n",
    "\n",
    "    clf = IsolationForest(max_samples=100, random_state=1337, )\n",
    "    clf.fit(X_train)\n",
    "    \n",
    "    y_pred_valid = clf.predict(X_valid)\n",
    "    y_pred_valid[y_pred_valid == 1] = 0\n",
    "    y_pred_valid[y_pred_valid == -1] = 1\n",
    "    \n",
    "    y_pred_test = clf.predict(bot_test_flows)\n",
    "    y_pred_test[y_pred_test == 1] = 0\n",
    "    y_pred_test[y_pred_test == -1] = 1\n",
    "    y_test = np.ones((bot_test_flows.shape[0], ))\n",
    "    print(\"Precision and Recall for scenario 9, 10, 11\")\n",
    "    print(precision_score(y_valid, y_pred_valid))\n",
    "    print(recall_score(y_valid, y_pred_valid))\n",
    "    print(\"Recall for unseen botnet flows in scenario 12\")\n",
    "    print(recall_score(y_valid, y_pred_valid))\n",
    "    print(\"------------------------------------\\n Next fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential vs non-sequential methods is an interesting experiment.\n",
    "\n",
    "Let us first look at the results. The non-sequential models do quite poorly considering that they had all the data from several scenarios to learn from. This is because they do not leverage the power of sequential data\n",
    "\n",
    "We have see that the sequence-based models show promising results given that the ngram flows we analysed here were discretized using a combination of just two features. Sequential models leverage online learning based on a stream netflows. In the sketches and hash based approaches, we can approximate the flow profiles in reasonable time and without considerable memory overhead. Additionally we should not overlook that fact that in our profiling approach, we try to learn from the botnet flow of one host to classify another type; which is a difficult setting given the data. \n",
    "\n",
    "In conclusion, given a scenario with limited data, we do not expect the non-sequential model to perform well enough. Though in our experiment, we use all data available, in the limited setting it will not do so well. Furthermore, as suggested earlier, if the scenario is such that we must tackle botnets in real time as the stream of data flows in, non-sequential methods are quite useless as they are built on assumptions of IID. This is reasonably well tackled by using sequential approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('env': venv)",
   "language": "python",
   "name": "python38264bitenvvenv7b9860dd079f4740a146ad2038fa480b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
